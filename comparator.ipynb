{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeep/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sandeep/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Response():\n",
    "    def __init__(self,data):\n",
    "        self.data=set(data)\n",
    "\n",
    "    def getMapping(self,file):\n",
    "        '''\n",
    "         Prints out the corresponding document names from list of document IDs\n",
    "        '''\n",
    "        self.mapping=json.load(open(file))\n",
    "        data= list(map(lambda i:(i,self.mapping[i]),self.data)) # list of tuples consisting of document id and their location\n",
    "        data.sort()\n",
    "        \n",
    "        print(tabulate(data,headers=['Document ID','Location']))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(len(self.data))\n",
    "\n",
    "    def add(self,response):\n",
    "        '''\n",
    "        Performs the set union of given object and pass object\n",
    "        '''\n",
    "        return Response(set.union(self.data,response.data))\n",
    "        \n",
    "    def intersect(self,response):\n",
    "        '''\n",
    "        Performs the set intersection of given object with pass object\n",
    "        '''\n",
    "        return Response(set.intersection(self.data,response.data))\n",
    "\n",
    "    def diff(self,response):\n",
    "        '''\n",
    "        Performs the set difference of given object with pass object\n",
    "        '''\n",
    "        return Response(set.difference(self.data,response.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Query():\n",
    "    def __init__(self,file):\n",
    "        '''\n",
    "        initializes the object with loading the index file\n",
    "        '''\n",
    "        self.db=json.load(open(file))\n",
    "        self.db=defaultdict(lambda:[],self.db)        \n",
    "\n",
    "    def OR(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying OR operation on given list of documents\n",
    "        '''\n",
    "        return term1.add(term2)\n",
    "    \n",
    "    def AND(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying AND operation on given list of documents\n",
    "        '''\n",
    "        return term1.intersect(term2)\n",
    "    \n",
    "    def ANDNOT(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying AND NOT operation on given list of documents\n",
    "        '''\n",
    "        univ=Response(np.arange(467))\n",
    "        not_term2=univ.diff(term2)\n",
    "        return term1.intersect(not_term2)\n",
    "\n",
    "    def ORNOT(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying OR NOT operation on given list of documents\n",
    "        '''\n",
    "        univ=Response(np.arange(467))\n",
    "        not_term2=univ.diff(term2)\n",
    "        joint = term1.intersect(term2)\n",
    "        return not_term2.add(joint)\n",
    "\n",
    "    def count(self,first,second):\n",
    "        i,j,count=0,0,0\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                j+=1\n",
    "            else:\n",
    "                i+=1\n",
    "                j+=1\n",
    "        return count\n",
    "\n",
    "    def no_comparisonsOR(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in OR operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        second =list(term2.data)\n",
    "        second.sort()\n",
    "        return self.count(first,second)    \n",
    "\n",
    "    def no_comparisonsAND(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in AND operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        second =list(term2.data)\n",
    "        second.sort()\n",
    "        return self.count(first,second) \n",
    "    \n",
    "    def no_comparisonsANDNOT(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in AND NOT operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        univ=Response(np.arange(467)) \n",
    "        not_term2=univ.diff(term2) \n",
    "        second=list(not_term2.data)\n",
    "        second.sort()\n",
    "        return self.count(first,second) \n",
    "    \n",
    "    def no_comparisonsORNOT(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in OR NOT operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        univ=Response(np.arange(467)) \n",
    "        not_term2=univ.diff(term2) \n",
    "        second=list(not_term2.data)\n",
    "        second.sort()\n",
    "    \n",
    "        return self.count(first,second)  \n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        # stemmedWords = list([stemmer.stem(word) for word in text_tokens])\n",
    "        # validTokens = [i for i in stemmedWords if i not in stopWords]\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens\n",
    "    \n",
    "    def processQuery(self,inp,ops):\n",
    "        '''\n",
    "        Performs query with given string and list of operations\n",
    "        '''\n",
    "        terms=self.preProcess(inp)\n",
    "        print(terms)\n",
    "        print(ops)\n",
    "        output=Response(self.db[terms[0]])\n",
    "        comparisons=0\n",
    "        for i in tnrange(1,len(terms)):\n",
    "            curr=Response(self.db[terms[i]])\n",
    "            if(ops[i-1]=='OR'):\n",
    "                output=self.OR(output, curr)\n",
    "                comparisons+=self.no_comparisonsOR(output,curr)\n",
    "            elif(ops[i-1]=='AND'):\n",
    "                output=self.AND(output, curr)\n",
    "                comparisons+=self.no_comparisonsAND(output,curr)\n",
    "            elif(ops[i-1]=='OR NOT'):\n",
    "                output=self.ORNOT(output, curr)\n",
    "                comparisons+=self.no_comparisonsORNOT(output,curr)\n",
    "            elif(ops[i-1]=='AND NOT'):\n",
    "                output=self.ANDNOT(output, curr)\n",
    "                comparisons+=self.no_comparisonsANDNOT(output,curr)\n",
    "            else:\n",
    "                raise Exception(\"Operand not Identified:\"+ops[i-1])\n",
    "\n",
    "        print(\"Number of documents matched:\",output)\n",
    "        print(\"No. of comparisons required:\",comparisons)\n",
    "    \n",
    "        # output.getMapping('mapping_25k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ingredients', 'bottom']\n['AND']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5bfb3e1ec6544cba78ed048d3f677c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nNumber of documents matched: 3230\nNo. of comparisons required: 3230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(input(\"Enter the number of queries:\"))\n",
    "for _ in range(n):\n",
    "    sentence_query = input(\"Enter the sentence:-\")\n",
    "    \n",
    "    # Performing preprocessing(splitting, uppercase, stripping space from endpoints) on the operand input\n",
    "    operands_=list(map(str.strip,input(\"Enter the operands:-\").upper().split(\",\")))\n",
    "\n",
    "    query=Query(\"output_25k.json\")\n",
    "    query.processQuery(sentence_query,operands_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}